# -*- coding: utf-8 -*-
"""funciones_auxiliares.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UWiT02bwKCQaNuRr-vqqbldKB22X41DZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Union, Optional
from scipy.stats import pearsonr

from statsmodels.tsa.stattools import adfuller
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import STL

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.stats.diagnostic import acorr_ljungbox
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
from statsmodels.tsa.stattools import adfuller, kpss
try:
    from arch.unitroot import PhillipsPerron
    HAVE_PP = True
except Exception:
    HAVE_PP = False

def verificar_fechas_faltantes(df: pd.DataFrame,
                              columna_fecha: str = 'fecha',
                              freq: str = 'D',
                              fecha_inicio: Optional[Union[str, pd.Timestamp]] = None,
                              fecha_fin: Optional[Union[str, pd.Timestamp]] = None,
                              mostrar_resultado: bool = True) -> pd.DatetimeIndex:
    """
    Verifica si faltan fechas en un rango específico dentro de un DataFrame.

    Parámetros:
    -----------
    df : pd.DataFrame
        DataFrame que contiene la columna de fechas a verificar
    columna_fecha : str, default 'fecha'
        Nombre de la columna que contiene las fechas
    freq : str, default 'D'
        Frecuencia para el rango de fechas ('D'=diario, 'H'=horario, 'M'=mensual, etc.)
    fecha_inicio : str, pd.Timestamp, o None, default None
        Fecha de inicio del rango. Si es None, usa el mínimo de la columna
    fecha_fin : str, pd.Timestamp, o None, default None
        Fecha de fin del rango. Si es None, usa el máximo de la columna
    mostrar_resultado : bool, default True
        Si True, imprime el resultado en consola

    Retorna:
    --------
    pd.DatetimeIndex
        Índice con las fechas faltantes (vacío si no falta ninguna fecha)

    Ejemplos:
    ---------
    # Uso básico
    fechas_faltantes = verificar_fechas_faltantes(df)

    # Especificar rango personalizado
    fechas_faltantes = verificar_fechas_faltantes(
        df,
        columna_fecha='timestamp',
        fecha_inicio='2023-01-01',
        fecha_fin='2023-12-31'
    )

    # Verificar por horas
    fechas_faltantes = verificar_fechas_faltantes(df, freq='H')
    """

    # Validar que la columna existe
    if columna_fecha not in df.columns:
        raise ValueError(f"La columna '{columna_fecha}' no existe en el DataFrame")

    # Asegurarse de que la columna sea tipo datetime
    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):
        df_temp = df.copy()
        df_temp[columna_fecha] = pd.to_datetime(df_temp[columna_fecha])
    else:
        df_temp = df

    # Determinar fechas de inicio y fin
    inicio = pd.to_datetime(fecha_inicio) if fecha_inicio is not None else df_temp[columna_fecha].min()
    fin = pd.to_datetime(fecha_fin) if fecha_fin is not None else df_temp[columna_fecha].max()

    # Obtener rango completo de fechas
    rango_fechas = pd.date_range(start=inicio, end=fin, freq=freq)

    # Encontrar fechas faltantes
    fechas_en_df = pd.to_datetime(df_temp[columna_fecha].unique())
    fechas_faltantes = rango_fechas.difference(fechas_en_df)

    # Mostrar resultado si se solicita
    if mostrar_resultado:
        if fechas_faltantes.empty:
            print(f"✅ El dataframe contiene todas las fechas entre {inicio.date()} y {fin.date()}")
        else:
            print(f"⚠️ Faltan {len(fechas_faltantes)} fechas en el dataframe:")
            print("Primeras 10 fechas faltantes:")
            for fecha in fechas_faltantes[:10]:
                print(f"  - {fecha.date()}")
            if len(fechas_faltantes) > 10:
                print(f"  ... y {len(fechas_faltantes) - 10} fechas más")

    return fechas_faltantes


# Función auxiliar para completar fechas faltantes
def completar_fechas_faltantes(df: pd.DataFrame,
                              columna_fecha: str = 'fecha',
                              freq: str = 'D',
                              metodo_relleno: str = 'ffill',
                              fecha_inicio: Optional[Union[str, pd.Timestamp]] = None,
                              fecha_fin: Optional[Union[str, pd.Timestamp]] = None) -> pd.DataFrame:
    """
    Completa las fechas faltantes en un DataFrame.

    Parámetros:
    -----------
    df : pd.DataFrame
        DataFrame original
    columna_fecha : str
        Nombre de la columna de fechas
    freq : str
        Frecuencia de las fechas
    metodo_relleno : str
        Método para rellenar valores faltantes ('ffill', 'bfill', 'interpolate', None)
    fecha_inicio, fecha_fin : opcional
        Rango personalizado de fechas

    Retorna:
    --------
    pd.DataFrame
        DataFrame con fechas completadas
    """

    # Asegurar tipo datetime
    df_temp = df.copy()
    if not pd.api.types.is_datetime64_any_dtype(df_temp[columna_fecha]):
        df_temp[columna_fecha] = pd.to_datetime(df_temp[columna_fecha])

    # Determinar rango
    inicio = pd.to_datetime(fecha_inicio) if fecha_inicio is not None else df_temp[columna_fecha].min()
    fin = pd.to_datetime(fecha_fin) if fecha_fin is not None else df_temp[columna_fecha].max()

    # Crear rango completo
    rango_completo = pd.date_range(start=inicio, end=fin, freq=freq)

    # Reindexar con el rango completo
    df_temp = df_temp.set_index(columna_fecha).reindex(rango_completo)
    df_temp.index.name = columna_fecha
    df_temp = df_temp.reset_index()

    # Aplicar método de relleno si se especifica
    if metodo_relleno == 'ffill':
        df_temp = df_temp.fillna(method='ffill')
    elif metodo_relleno == 'bfill':
        df_temp = df_temp.fillna(method='bfill')
    elif metodo_relleno == 'interpolate':
        df_temp = df_temp.interpolate()

    return df_temp



# --- FUNCIÓN PARA REEMPLAZO DE OUTLIERS ---
def replace_outliers(data, method='winsorize', quantile_range=(0.01, 0.99)):
    """
    Reemplaza outliers usando diferentes métodos

    Parameters:
    - data: Series de pandas con los datos
    - method: 'winsorize', 'cap', 'median', 'mean', 'remove'
    - quantile_range: tupla con los percentiles para winsorize/cap

    Returns:
    - data_clean: Series con outliers reemplazados
    - outlier_info: diccionario con información sobre los outliers
    """
    data_clean = data.copy()

    # Método IQR para detectar outliers
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identificar outliers
    outlier_mask = (data < lower_bound) | (data > upper_bound)
    outliers = data[outlier_mask]

    if method == 'winsorize':
        # Reemplazar con percentiles
        lower_perc = data.quantile(quantile_range[0])
        upper_perc = data.quantile(quantile_range[1])
        data_clean = data.clip(lower=lower_perc, upper=upper_perc)

    elif method == 'cap':
        # Reemplazar con límites IQR
        data_clean = data.clip(lower=lower_bound, upper=upper_bound)

    elif method == 'median':
        # Reemplazar outliers con la mediana
        median_val = data.median()
        data_clean[outlier_mask] = median_val

    elif method == 'mean':
        # Reemplazar outliers con la media
        mean_val = data.mean()
        data_clean[outlier_mask] = mean_val

    elif method == 'remove':
        # Eliminar outliers (convertir a NaN)
        data_clean[outlier_mask] = np.nan

    # Información sobre outliers
    outlier_info = {
        'original_outliers': len(outliers),
        'outlier_percentage': len(outliers) / len(data) * 100,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'method_used': method
    }

    return data_clean, outlier_info

# --- FUNCIÓN PARA CALCULAR CORRELACIONES CON LAGS ---
def calculate_lagged_correlations(df, variables, lags):
    """
    Calcula correlaciones con diferentes lags entre variables

    Parameters:
    - df: DataFrame con las series temporales
    - variables: lista de nombres de variables
    - lags: lista de lags a calcular

    Returns:
    - dict con matrices de correlación para cada lag
    """
    lagged_correlations = {}

    for lag in lags:
        # Crear DataFrame con variables lagged
        df_lagged = pd.DataFrame(index=df.index)

        # Variables originales (tiempo t)
        for var in variables:
            df_lagged[f'{var}_t0'] = df[var]

        # Variables con lag (tiempo t-lag)
        for var in variables:
            df_lagged[f'{var}_t-{lag}'] = df[var].shift(lag)

        # Calcular matriz de correlación
        corr_matrix = df_lagged.corr()
        lagged_correlations[lag] = corr_matrix

        # También guardar solo las correlaciones cruzadas (original vs lagged)
        cross_corr = pd.DataFrame(index=[f'{v}_t0' for v in variables],
                                 columns=[f'{v}_t-{lag}' for v in variables])

        for i, var1 in enumerate(variables):
            for j, var2 in enumerate(variables):
                cross_corr.iloc[i, j] = corr_matrix.loc[f'{var1}_t0', f'{var2}_t-{lag}']

        lagged_correlations[f'cross_{lag}'] = cross_corr

    return lagged_correlations

# --- FUNCIÓN PARA PROBAR DIFERENTES MÉTODOS ---
def compare_outlier_methods(data, column_name):
    """
    Compara diferentes métodos de manejo de outliers para una columna
    """
    methods = ['winsorize', 'cap', 'median', 'mean']
    results = {}

    original = data[column_name].dropna()

    print(f"\nComparación de métodos para {column_name}:")
    print("-" * 50)

    for method in methods:
        clean_data, info = replace_outliers(original, method=method)

        results[method] = {
            'mean': clean_data.mean(),
            'std': clean_data.std(),
            'skewness': clean_data.skew(),
            'outliers_removed': info['original_outliers']
        }

        print(f"{method:>10}: Mean={clean_data.mean():7.2f}, Std={clean_data.std():7.2f}, "
              f"Skew={clean_data.skew():6.3f}, Outliers={info['original_outliers']}")

    return results


def run_adf(x, regression="c", autolag="AIC"):
    """
    regression: "c" (constante), "ct" (constante+tendencia), "nc" (sin constante)
    """
    res = adfuller(x, regression=regression, autolag=autolag)
    out = {
        "stat": res[0],
        "pvalue": res[1],
        "lags": res[2],
        "nobs": res[3],
        "crit_1%": res[4].get("1%"),
        "crit_5%": res[4].get("5%"),
        "crit_10%": res[4].get("10%"),
        "regression": regression,
    }
    return out

def run_kpss_test(x, regression="c", nlags="auto"):
    """
    regression: "c" (nivel), "ct" (tendencia)
    """
    stat, pvalue, lags, crit = kpss(x, regression=regression, nlags=nlags)
    return {
        "stat": stat,
        "pvalue": pvalue,
        "lags": lags,
        "crit_10%": crit.get("10%"),
        "crit_5%": crit.get("5%"),
        "crit_2.5%": crit.get("2.5%"),
        "crit_1%": crit.get("1%"),
        "regression": regression,
    }

def run_pp(x, trend="c"):
    """
    trend: "n"(none), "c"(const), "ct"(const+trend)
    """
    if not HAVE_PP:
        return None
    res = PhillipsPerron(x, trend=trend)
    return {
        "stat": res.stat,
        "pvalue": res.pvalue,
        "lags": res.lags,
        "trend": trend,
        "crit_1%": res.critical_values.get("1%"),
        "crit_5%": res.critical_values.get("5%"),
        "crit_10%": res.critical_values.get("10%"),
    }

def interpret_row(adf_c, adf_ct, kpss_c, kpss_ct, pp_c, pp_ct):
    """
    Regla simple:
    - ADF/PP (p < 0.05) => evidencia de estacionariedad.
    - KPSS (p < 0.05) => rechaza estacionariedad (evidencia de no estacionaria).
    """
    info = []
    if adf_c["pvalue"] < 0.05 or (pp_c and pp_c["pvalue"] < 0.05):
        info.append("ADF/PP con constante: sugiere estacionaria (rechaza raíz unitaria).")
    else:
        info.append("ADF/PP con constante: NO rechaza raíz unitaria (no estacionaria).")

    if adf_ct["pvalue"] < 0.05 or (pp_ct and pp_ct["pvalue"] < 0.05):
        info.append("ADF/PP con tendencia: sugiere estacionaria alrededor de tendencia.")
    else:
        info.append("ADF/PP con tendencia: NO rechaza raíz unitaria con tendencia.")

    if kpss_c["pvalue"] < 0.05:
        info.append("KPSS nivel: rechaza estacionariedad (no estacionaria en nivel).")
    else:
        info.append("KPSS nivel: no rechaza estacionariedad (podría ser estacionaria).")

    if kpss_ct["pvalue"] < 0.05:
        info.append("KPSS tendencia: rechaza estacionariedad (no estacionaria alrededor de tendencia).")
    else:
        info.append("KPSS tendencia: no rechaza estacionariedad con tendencia.")

    # Diagnóstico rápido
    if ((adf_c["pvalue"] < 0.05) or (pp_c and pp_c["pvalue"] < 0.05)) and (kpss_c["pvalue"] >= 0.05):
        diag = "Diagnóstico: estacionaria (nivel)."
    elif ((adf_ct["pvalue"] < 0.05) or (pp_ct and pp_ct["pvalue"] < 0.05)) and (kpss_ct["pvalue"] >= 0.05):
        diag = "Diagnóstico: estacionaria alrededor de tendencia."
    else:
        diag = "Diagnóstico: no estacionaria (sugerir diferenciar)."
    info.append(diag)
    return " | ".join(info)




# ---------- utilidades ----------
def infer_period(idx: pd.DatetimeIndex) -> int:
    if len(idx) < 3:
        return 7
    diffs_ns = np.diff(idx.view("int64"))
    if len(diffs_ns) == 0:
        return 7
    dt_days = np.median(diffs_ns) / (1e9 * 60 * 60 * 24)
    if 0.8 <= dt_days <= 1.2:   # diaria
        return 7
    if 27 <= dt_days <= 31:     # mensual
        return 12
    if 6 <= dt_days <= 8:       # semanal
        return 52
    return 7

def prepare_regular(x: pd.Series) -> pd.Series:
    x = x.copy()
    if not isinstance(x.index, pd.DatetimeIndex):
        x.index = pd.to_datetime(x.index, errors="coerce")
    x = x[~x.index.isna()].sort_index()

    try:
        freq = pd.infer_freq(x.index)
    except Exception:
        freq = None

    if freq is None:
        diffs_ns = np.diff(x.index.view("int64"))
        if len(diffs_ns) == 0:
            rule = "1D"
        else:
            step_days = int(pd.Series(diffs_ns / (1e9*60*60*24)).round().mode().iloc[0])
            step_days = max(step_days, 1)
            rule = f"{step_days}D"
    else:
        rule = freq

    xr = x.asfreq(rule)
    xr = xr.interpolate(method="time").bfill().ffill()
    return xr

def stl_decompose(s: pd.Series, period: int = None, robust: bool = True):
    s_reg = prepare_regular(s)
    per = period or infer_period(s_reg.index)
    res = STL(s_reg, period=per, robust=robust).fit()
    var_total = np.var(s_reg.values, ddof=1) if len(s_reg) > 1 else np.nan
    var_trend = np.var(res.trend.values, ddof=1)
    var_seas  = np.var(res.seasonal.values, ddof=1)
    var_res   = np.var(res.resid.values, ddof=1)
    pct_trend = 100*var_trend/var_total if var_total and var_total>0 else np.nan
    pct_seas  = 100*var_seas /var_total if var_total and var_total>0 else np.nan
    pct_res   = 100*var_res  /var_total if var_total and var_total>0 else np.nan
    return {
        "series_regular": s_reg,
        "trend": res.trend,
        "seasonal": res.seasonal,
        "resid": res.resid,
        "period": per,
        "pct_var": {"trend": pct_trend, "seasonal": pct_seas, "resid": pct_res}
    }

def plot_stl(name: str, result: dict):
    s = result["series_regular"]; trend = result["trend"]
    seas = result["seasonal"]; resid = result["resid"]; per = result["period"]
    fig, axes = plt.subplots(4, 1, figsize=(12, 8), sharex=True)
    axes[0].plot(s.index, s.values); axes[0].set_title(f"{name} (STL period={per}) - Serie")
    axes[0].grid(True)
    axes[1].plot(trend.index, trend.values); axes[1].set_title("Tendencia"); axes[1].grid(True)
    axes[2].plot(seas.index,  seas.values);  axes[2].set_title("Estacionalidad"); axes[2].grid(True)
    axes[3].plot(resid.index, resid.values); axes[3].axhline(0, ls="--", lw=1)
    axes[3].set_title("Residuo"); axes[3].grid(True)
    plt.tight_layout(); plt.show()


# Métricas
def mape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    mask = y_true != 0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

def smape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    mask = denom != 0
    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / denom[mask]) * 100

def to_datetime_index(df):
    df = df.copy()
    if "fecha" in df.columns:
        df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce")
        df = df.dropna(subset=["fecha"]).sort_values("fecha").set_index("fecha")
    if not isinstance(df.index, pd.DatetimeIndex):
        df.index = pd.to_datetime(df.index, errors="coerce")
        df = df[~df.index.isna()].sort_index()
    return df

def regularize_daily(s: pd.Series) -> pd.Series:
    """
    Asegura frecuencia diaria e imputa faltantes por interpolación temporal.
    """
    s = s.copy()
    s.index = pd.to_datetime(s.index, errors="coerce")
    s = s[~s.index.isna()].sort_index()
    s = s.asfreq("D")
    s = s.interpolate(method="time").bfill().ffill()
    return s.astype(float)
